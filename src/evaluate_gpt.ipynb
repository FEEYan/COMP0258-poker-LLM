{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import socket\n",
    "import torch\n",
    "import random\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_gpt4(client, model_name, content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        seed=53,\n",
    "        max_tokens=128,\n",
    "        temperature=0\n",
    "    )\n",
    "    rating = response.choices[0].message.content.strip()\n",
    "\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(client):\n",
    "    # input_json_file = args.input_json_file\n",
    "    testing_set = \"postflop\"\n",
    "    input_json_file = \"../data/poker-postflop/postflop_10k_test_set_prompt_and_label.json\"\n",
    "    model_name = \"gpt-4o\"\n",
    "\n",
    "    print(f\"Processing input JSON file: {input_json_file}\")\n",
    "\n",
    "    # Read JSON data\n",
    "    with open(input_json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # # For debugging purpose\n",
    "    data = data[0:5000]\n",
    "\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        item = data[i]\n",
    "        # Extract instruction (poker scenario) and expected output (optimal action)\n",
    "        instruction = item.get('instruction', '')\n",
    "        expected_output = item.get('output', '')\n",
    "        ground_truths.append(expected_output)\n",
    "\n",
    "        if instruction:  # Skip entries that don't have instructions\n",
    "            # print(f\"Processing poker scenario {len(predictions) + 1}\")\n",
    "            # print(instruction)\n",
    "\n",
    "            try:\n",
    "                # Generate model's response to the poker scenario\n",
    "                model_response = execute_gpt4(client, model_name, instruction)\n",
    "\n",
    "\n",
    "                # # Create result dictionary\n",
    "                # result = {\n",
    "                #     'instruction': instruction,\n",
    "                #     'expected_output': expected_output,\n",
    "                #     'model_response': model_response,\n",
    "                #     'correct': model_response.strip() == expected_output.strip()\n",
    "                # }\n",
    "\n",
    "            except Exception as e:\n",
    "                model_response = 'ERROR'\n",
    "                # result = {\n",
    "                #     'instruction': instruction,\n",
    "                #     'expected_output': expected_output,\n",
    "                #     'model_response': 'ERROR',\n",
    "                #     'correct': False\n",
    "                # }\n",
    "                print(f\"Failed to process poker scenario\")\n",
    "                print(f\"Error: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "\n",
    "            predictions.append(model_response)\n",
    "\n",
    "    # Create output dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Prediction\": predictions,\n",
    "        \"Ground Truth\": ground_truths\n",
    "    })\n",
    "\n",
    "    output_path = \"../testing-results\"\n",
    "    # output_path = \"./\"\n",
    "    # Save the DataFrames to CSV files\n",
    "    results_df.to_csv(\n",
    "        f\"{output_path}/{model_name}-{testing_set}_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    client = OpenAI(api_key='')\n",
    "    # process_json_file(client, args)\n",
    "    process_json_file(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mps\n",
      "Processing input JSON file: ../data/poker-postflop/postflop_10k_test_set_prompt_and_label.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [58:10<00:00,  1.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 00:58:10\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(\n",
    "    #     description=\"Sentiment Transfer using LLM ChatGPT\")\n",
    "    # parser.add_argument('--model_name', type=str,\n",
    "    #                     required=True, help='Pretrained LLM name',\n",
    "    #                     default='gpt-3.5-turbo')\n",
    "    # parser.add_argument('--input_json_file', required=True,\n",
    "    #                     help=\"Path to the input json file\",\n",
    "    #                     default='../data/poker-preflop/preflop_1k_test_set_prompt_and_label.json')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    start_time = time.time()\n",
    "    # print(\"Command-line arguments:\")\n",
    "    # for arg, value in vars(args).items():\n",
    "    #     print(f\"{arg}: {value}\")\n",
    "\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else (\n",
    "        'cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    print(f'Device {device}')\n",
    "\n",
    "    # if torch.cuda.is_available():\n",
    "    #     print(f'GPU machine name: {torch.cuda.get_device_name(0)}')\n",
    "    #     print(f'SSH machine name: {socket.gethostname()}')\n",
    "\n",
    "    set_seed(53)\n",
    "\n",
    "    # main(args)\n",
    "    main()\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken_seconds = end_time - start_time\n",
    "    time_taken_formatted = time.strftime(\n",
    "        '%H:%M:%S', time.gmtime(time_taken_seconds))\n",
    "\n",
    "    print(f\"Time taken: {time_taken_formatted}\")\n",
    "\n",
    "    print('=' * 50 + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
