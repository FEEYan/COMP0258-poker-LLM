{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action prediction accuracy: 0.2810\n",
      "Accuracy for fold: 0.1080 (27/250)\n",
      "Accuracy for check: 0.0000 (0/250)\n",
      "Accuracy for call: 0.9600 (240/250)\n",
      "Accuracy for raise: 0.0560 (14/250)\n",
      "RMSE for raise amounts: 44.4610\n",
      "NRMSE for raise amounts: 1.4820\n",
      "Action prediction accuracy: 0.3706\n",
      "Accuracy for fold: 0.4284 (1071/2500)\n",
      "Accuracy for check: 0.1348 (337/2500)\n",
      "Accuracy for call: 0.5364 (1341/2500)\n",
      "Accuracy for raise: 0.3703 (574/1550)\n",
      "Accuracy for bet: 0.4032 (383/950)\n",
      "RMSE for raise amounts: 90.7761\n",
      "NRMSE for raise amounts: 0.9867\n",
      "RMSE for bet amounts: 60.0036\n",
      "NRMSE for bet amounts: 1.5790\n",
      "Action prediction accuracy: 0.3936\n",
      "Accuracy for fold: 0.0193 (53/2750)\n",
      "Accuracy for check: 0.4007 (1102/2750)\n",
      "Accuracy for call: 0.8767 (2411/2750)\n",
      "Accuracy for raise: 0.1300 (234/1800)\n",
      "Accuracy for bet: 0.5579 (530/950)\n",
      "RMSE for raise amounts: 89.4485\n",
      "NRMSE for raise amounts: 1.0165\n",
      "RMSE for bet amounts: 37.3509\n",
      "NRMSE for bet amounts: 0.9829\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import argparse\n",
    "\n",
    "\n",
    "def clean_prediction(text):\n",
    "    \"\"\"Clean the prediction text by removing <end_of_turn> and <pad> tags.\"\"\"\n",
    "    text = text.replace(\"<end_of_turn>\\n<pad>\", \"\").strip()\n",
    "    text = text.replace(\"<end_of_turn>\", \"\").strip()\n",
    "    text = text.replace(\" chips\", \"\").strip()\n",
    "    text = text.replace(\".\", \"\").strip()\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def extract_action(move):\n",
    "    \"\"\"Extract the action (call, fold, check, raise) from a move.\"\"\"\n",
    "    if pd.isna(move):\n",
    "        return np.nan\n",
    "    if \"raise\" in move:\n",
    "        return \"raise\"\n",
    "    elif \"call\" in move:\n",
    "        return \"call\"\n",
    "    elif \"check\" in move:\n",
    "        return \"check\"\n",
    "    elif \"fold\" in move:\n",
    "        return \"fold\"\n",
    "    elif \"bet\" in move:\n",
    "        return \"bet\"\n",
    "    else:\n",
    "        return move\n",
    "\n",
    "\n",
    "def extract_amount(move):\n",
    "    \"\"\"Extract the bet amount from a raise or bet move.\"\"\"\n",
    "    if pd.isna(move):\n",
    "        return np.nan\n",
    "    if \"raise\" in move:\n",
    "        try:\n",
    "            return float(move.split(\"raise\")[1].strip())\n",
    "        except:\n",
    "            return 0\n",
    "    elif \"bet\" in move:\n",
    "        try:\n",
    "            return float(move.split(\"bet\")[1].strip())\n",
    "        except:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def main(model_name, testing_type):\n",
    "    # Load the CSV file based on the model name\n",
    "    file_path = f'/Users/weber/Github/COMP0258/testing-results/{model_name}_predictions.csv'\n",
    "    if file_path is None:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "    # Clean predictions\n",
    "    df['Prediction_Clean'] = df['Prediction'].apply(clean_prediction)\n",
    "    df['Ground_Truth_Clean'] = df['Ground Truth']\n",
    "\n",
    "    # Extract action and amount\n",
    "    df['Pred_Action'] = df['Prediction_Clean'].apply(extract_action)\n",
    "    df['True_Action'] = df['Ground_Truth_Clean'].apply(extract_action)\n",
    "    df['Pred_Amount'] = df['Prediction_Clean'].apply(extract_amount)\n",
    "    df['True_Amount'] = df['Ground_Truth_Clean'].apply(extract_amount)\n",
    "\n",
    "    # save the cleaned dataframe\n",
    "    cleaned_file_path = f'/Users/weber/Github/COMP0258/testing-results/{model_name}_cleaned.csv'\n",
    "    df[['Pred_Action', 'True_Action', 'Pred_Amount', 'True_Amount']].to_csv(\n",
    "        cleaned_file_path, index=False)\n",
    "\n",
    "    # Calculate overall accuracy for the action type\n",
    "    action_accuracy = accuracy_score(df['True_Action'], df['Pred_Action'])\n",
    "    print(f\"Action prediction accuracy: {action_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "    # Confusion matrix for actions\n",
    "    if testing_type == 'preflop':\n",
    "        actions = ['fold', 'check', 'call', 'raise']\n",
    "    else:\n",
    "        actions = ['fold', 'check', 'call', 'raise', 'bet']\n",
    "    action_df = df[df['True_Action'].isin(\n",
    "        actions) & df['Pred_Action'].isin(actions)]\n",
    "    cm = confusion_matrix(\n",
    "        action_df['True_Action'], action_df['Pred_Action'], labels=actions)\n",
    "    cm_df = pd.DataFrame(cm, index=actions, columns=actions)\n",
    "\n",
    "    # Calculate per-action accuracy\n",
    "    action_specific_accuracy = {}\n",
    "    for action in actions:\n",
    "        action_rows = df[df['True_Action'] == action]\n",
    "        if len(action_rows) > 0:\n",
    "            correct = sum(action_rows['Pred_Action'] == action)\n",
    "            accuracy = correct / len(action_rows)\n",
    "            action_specific_accuracy[action] = accuracy\n",
    "            print(\n",
    "                f\"Accuracy for {action}: {accuracy:.4f} ({correct}/{len(action_rows)})\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    # adding title for the confusion matrix\n",
    "    plt.suptitle(f'{model_name}', fontsize=20)\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix for Poker Actions')\n",
    "    plt.ylabel('True Action')\n",
    "    plt.xlabel('Predicted Action')\n",
    "\n",
    "    # Analyze raise and bet amount accuracy\n",
    "    # For raises\n",
    "    raise_df = df[(df['True_Action'] == 'raise') &\n",
    "                  (df['Pred_Action'] == 'raise')]\n",
    "    raise_df = raise_df.dropna(subset=['True_Amount', 'Pred_Amount'])\n",
    "\n",
    "    # For bets\n",
    "    bet_df = df[(df['True_Action'] == 'bet') &\n",
    "                (df['Pred_Action'] == 'bet')]\n",
    "    bet_df = bet_df.dropna(subset=['True_Amount', 'Pred_Amount'])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    # Calculate metrics and plot for raises\n",
    "    if len(raise_df) > 0:\n",
    "        # Calculate RMSE for raise amounts\n",
    "        raise_rmse = np.sqrt(\n",
    "            ((raise_df['True_Amount'] - raise_df['Pred_Amount']) ** 2).mean())\n",
    "        print(f\"RMSE for raise amounts: {raise_rmse:.4f}\")\n",
    "\n",
    "        # Calculate NRMSE for raise amounts\n",
    "        if raise_df['True_Amount'].max() != raise_df['True_Amount'].min():\n",
    "            true_amount_range = raise_df['True_Amount'].max(\n",
    "            ) - raise_df['True_Amount'].min()\n",
    "            raise_nrmse = raise_rmse / true_amount_range\n",
    "            print(f\"NRMSE for raise amounts: {raise_nrmse:.4f}\")\n",
    "\n",
    "        # Plot raise amount comparison\n",
    "        plt.scatter(raise_df['True_Amount'],\n",
    "                    raise_df['Pred_Amount'], alpha=0.6, color='blue', label='Raise')\n",
    "\n",
    "    # Calculate metrics and plot for bets\n",
    "    if len(bet_df) > 0 and testing_type != 'preflop':\n",
    "        # Calculate RMSE for bet amounts\n",
    "        bet_rmse = np.sqrt(\n",
    "            ((bet_df['True_Amount'] - bet_df['Pred_Amount']) ** 2).mean())\n",
    "        print(f\"RMSE for bet amounts: {bet_rmse:.4f}\")\n",
    "\n",
    "        # Calculate NRMSE for bet amounts\n",
    "        if bet_df['True_Amount'].max() != bet_df['True_Amount'].min():\n",
    "            true_amount_range = bet_df['True_Amount'].max(\n",
    "            ) - bet_df['True_Amount'].min()\n",
    "            bet_nrmse = bet_rmse / true_amount_range\n",
    "            print(f\"NRMSE for bet amounts: {bet_nrmse:.4f}\")\n",
    "\n",
    "        # Plot bet amount comparison\n",
    "        plt.scatter(bet_df['True_Amount'],\n",
    "                    bet_df['Pred_Amount'], alpha=0.6, color='green', label='Bet')\n",
    "\n",
    "    # Add perfect prediction line if either raises or bets exist\n",
    "    if len(raise_df) > 0 or len(bet_df) > 0:\n",
    "        # Combine dataframes for determining overall min and max\n",
    "        combined_df = pd.concat([raise_df, bet_df])\n",
    "\n",
    "        if len(combined_df) > 0:\n",
    "            max_val = max(combined_df['True_Amount'].max(),\n",
    "                          combined_df['Pred_Amount'].max())\n",
    "            min_val = min(combined_df['True_Amount'].min(),\n",
    "                          combined_df['Pred_Amount'].min())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val],\n",
    "                     'r--', label='Perfect Prediction')\n",
    "\n",
    "            plt.title('Amount Prediction: Predicted vs True')\n",
    "            plt.xlabel('True Amount')\n",
    "            plt.ylabel('Predicted Amount')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "\n",
    "    # Save the combined image\n",
    "    combined_image_path = f'/Users/weber/Github/COMP0258/testing-results/{model_name}_confusion-matrix.png'\n",
    "\n",
    "    # Add accuracy information to the plot\n",
    "    accuracy_text = f\"Action accuracy: {action_accuracy:.2f}\"\n",
    "\n",
    "    # Add per-action accuracy to the text\n",
    "    for action in actions:\n",
    "        if action in action_specific_accuracy:\n",
    "            accuracy_text += f\" | {action.capitalize()} accuracy: {action_specific_accuracy[action]:.2f}\"\n",
    "\n",
    "    # Add RMSE to the text if available\n",
    "    if len(raise_df) > 0:\n",
    "        accuracy_text += f\" | RMSE for raise amounts: {raise_rmse:.2f}\"\n",
    "    if testing_type != 'preflop':\n",
    "        accuracy_text += f\" | RMSE for bet amounts: {bet_rmse:.2f}\"\n",
    "    plt.figtext(0.5, 0.01, accuracy_text, ha=\"center\", fontsize=12)\n",
    "\n",
    "    plt.savefig(combined_image_path)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(\n",
    "    #     description='Evaluate poker move predictions.')\n",
    "    # parser.add_argument('model_name', type=str,\n",
    "    #                     help='The name of the model to evaluate (e.g., gemma-2-9b-it, lora-gemma-2-9b-it)')\n",
    "    # args = parser.parse_args()\n",
    "    # main(args.model_name)\n",
    "    models = [\n",
    "        # 'gpt-4o-preflop', 'gpt-4o-postflop',\n",
    "        # 'gemma-2-9b-it-preflop', 'gemma-2-9b-it-postflop'\n",
    "        # 'lora-gemma-2-9b-it-preflop', 'lora-gemma-2-9b-it-postflop',\n",
    "        # 'Qwen2.5-7B-Instruct-1M-preflop', 'Qwen2.5-7B-Instruct-1M-postflop',\n",
    "        # 'lora-Qwen2.5-7B-Instruct-1M-preflop', 'lora-Qwen2.5-7B-Instruct-1M-postflop',\n",
    "        # 'lora_Qwen2.5_14B_model-5000-preflop', 'lora_Qwen2.5_14B_model-5000-postflop',\n",
    "        # \"qwen2.5-14b-instruct-bnb-4bit-preflop\", \"qwen2.5-14b-instruct-bnb-4bit-postflop\",\n",
    "        'Meta-Llama-3-8B-Instruct-preflop', 'Meta-Llama-3-8B-Instruct-postflop', 'Meta-Llama-3.1-8B-Instruct-total',\n",
    "        # 'Meta-Llama-3.1-8B-Instruct-preflop', 'Meta-Llama-3.1-8B-Instruct-postflop', 'Meta-Llama-3.1-8B-Instruct-total',\n",
    "        # 'lora-Meta-Llama-3.1-8B-Instruct-preflop', 'lora-Meta-Llama-3.1-8B-Instruct-postflop'\n",
    "        # 'Meta-Llama-3-8B-Instruct-preflop', 'Meta-Llama-3-8B-Instruct-postflop',\n",
    "        # 'lora-Meta-Llama-3-8B-Instruct-lr-6-preflop', 'lora-Meta-Llama-3-8B-Instruct-lr-6-postflop',\n",
    "        # 'lora-Llama-3.2-3B-Instruct-lr-5-preflop', 'lora-Llama-3.2-3B-Instruct-lr-5-postflop', 'lora-Llama-3.2-3B-Instruct-lr-5-total',\n",
    "        # 'lora-Llama-3.2-3B-Instruct-lr-6-preflop', 'lora-Llama-3.2-3B-Instruct-lr-6-postflop', \n",
    "        # 'Llama-3.2-3B-Instruct-preflop', 'Llama-3.2-3B-Instruct-postflop',\n",
    "        # 'lora_llama_3.1_8B_model-1600', 'lora_llama_3.1_8B_model-2400', 'lora_llama_3.1_8B_model-3200', 'lora_llama_3.1_8B_model-4000',\n",
    "        # 'lora_llama_3.2-3B_model-500', 'lora_llama_3.2-3B_model-1000',\n",
    "        ]\n",
    "    for model in models:\n",
    "        main(model, model.split('-')[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
