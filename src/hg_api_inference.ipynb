{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/LLM/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from safetensors.torch import load_file, save_file\n",
    "from huggingface_hub import InferenceApi, InferenceClient, upload_folder, create_repo\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 設定路徑 ===\n",
    "# 原始 MLX LoRA 檔案\n",
    "mlx_path = \"../adapters/lora-Llama-3.2-3B-Instruct-lr-6/adapters.safetensors\"\n",
    "output_dir = \"../converted_adapters\"  # 轉換後儲存目錄\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === 載入 MLX 權重 ===\n",
    "print(f\"🔄 讀取 MLX LoRA 權重檔：{mlx_path}\")\n",
    "mlx_tensors = load_file(mlx_path)\n",
    "\n",
    "# === 轉存為 Hugging Face 格式 ===\n",
    "adapter_model_path = os.path.join(output_dir, \"adapter_model.safetensors\")\n",
    "save_file(mlx_tensors, adapter_model_path)\n",
    "print(f\"✅ 儲存轉換後的權重到：{adapter_model_path}\")\n",
    "\n",
    "# === 建立 adapter_config.json ===\n",
    "adapter_config = {\n",
    "    \"peft_type\": \"LORA\",\n",
    "    \"base_model_name_or_path\": \"google/gemma-2-9b-it\",\n",
    "    \"inference_mode\": True,\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\"\n",
    "}\n",
    "\n",
    "adapter_config_path = os.path.join(output_dir, \"adapter_config.json\")\n",
    "with open(adapter_config_path, \"w\") as f:\n",
    "    json.dump(adapter_config, f, indent=4)\n",
    "\n",
    "print(f\"✅ 建立 adapter_config.json：{adapter_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "lora_adapter_path = \"../converted_adapters\"\n",
    "output_path = \"../models/lora-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_id=lora_adapter_path\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "model.to(device)\n",
    "\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(\"模型與 tokenizer 已儲存完畢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/weber50432/lora-Llama-3.2-3B-Instruct/commit/11dc56166ac47baf7d120b170e9a5c73728d7314', commit_message='Upload folder using huggingface_hub', commit_description='', oid='11dc56166ac47baf7d120b170e9a5c73728d7314', pr_url=None, repo_url=RepoUrl('https://huggingface.co/weber50432/lora-Llama-3.2-3B-Instruct', endpoint='https://huggingface.co', repo_type='model', repo_id='weber50432/lora-Llama-3.2-3B-Instruct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_folder(\n",
    "    repo_id=\"weber50432/lora-Llama-3.2-3B-Instruct\",\n",
    "    folder_path=\"../models/lora-Llama-3.2-3B-Instruct\",\n",
    "    path_in_repo=\".\",  # 上傳整包內容到根目錄\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your model repository ID\n",
    "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# read API token from json file\n",
    "with open(\"hg-api-key.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    token = data[\"key\"]\n",
    "\n",
    "\n",
    "def call_hf(prompt, max_new_tokens, temperature=0):\n",
    "    \"\"\"\n",
    "    Wrapper function to query Hugging Face Inference API.\n",
    "    \"\"\"\n",
    "    client_hf = InferenceClient(api_key=token)\n",
    "\n",
    "    response = client_hf.chat_completion(\n",
    "        model=repo_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question of the meaning of life is one of the most profound and debated topics in human history. It has been explored by philosophers, theologians, scientists, and many others across various cultures and disciplines. While there is no one definitive answer, here'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the meaning of life?\"\n",
    "max_new_tokens = 50\n",
    "call_hf(prompt, max_new_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
