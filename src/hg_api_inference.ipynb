{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/LLM/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from safetensors.torch import load_file, save_file\n",
    "from huggingface_hub import InferenceApi, InferenceClient, upload_folder, create_repo\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === è¨­å®šè·¯å¾‘ ===\n",
    "# åŸå§‹ MLX LoRA æª”æ¡ˆ\n",
    "mlx_path = \"../adapters/lora-Llama-3.2-3B-Instruct-lr-6/adapters.safetensors\"\n",
    "output_dir = \"../converted_adapters\"  # è½‰æ›å¾Œå„²å­˜ç›®éŒ„\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === è¼‰å…¥ MLX æ¬Šé‡ ===\n",
    "print(f\"ğŸ”„ è®€å– MLX LoRA æ¬Šé‡æª”ï¼š{mlx_path}\")\n",
    "mlx_tensors = load_file(mlx_path)\n",
    "\n",
    "# === è½‰å­˜ç‚º Hugging Face æ ¼å¼ ===\n",
    "adapter_model_path = os.path.join(output_dir, \"adapter_model.safetensors\")\n",
    "save_file(mlx_tensors, adapter_model_path)\n",
    "print(f\"âœ… å„²å­˜è½‰æ›å¾Œçš„æ¬Šé‡åˆ°ï¼š{adapter_model_path}\")\n",
    "\n",
    "# === å»ºç«‹ adapter_config.json ===\n",
    "adapter_config = {\n",
    "    \"peft_type\": \"LORA\",\n",
    "    \"base_model_name_or_path\": \"google/gemma-2-9b-it\",\n",
    "    \"inference_mode\": True,\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\"\n",
    "}\n",
    "\n",
    "adapter_config_path = os.path.join(output_dir, \"adapter_config.json\")\n",
    "with open(adapter_config_path, \"w\") as f:\n",
    "    json.dump(adapter_config, f, indent=4)\n",
    "\n",
    "print(f\"âœ… å»ºç«‹ adapter_config.jsonï¼š{adapter_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "lora_adapter_path = \"../converted_adapters\"\n",
    "output_path = \"../models/lora-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    model_id=lora_adapter_path\n",
    ")\n",
    "model = model.merge_and_unload()\n",
    "model.to(device)\n",
    "\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(\"æ¨¡å‹èˆ‡ tokenizer å·²å„²å­˜å®Œç•¢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/weber50432/lora-Llama-3.2-3B-Instruct/commit/11dc56166ac47baf7d120b170e9a5c73728d7314', commit_message='Upload folder using huggingface_hub', commit_description='', oid='11dc56166ac47baf7d120b170e9a5c73728d7314', pr_url=None, repo_url=RepoUrl('https://huggingface.co/weber50432/lora-Llama-3.2-3B-Instruct', endpoint='https://huggingface.co', repo_type='model', repo_id='weber50432/lora-Llama-3.2-3B-Instruct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_folder(\n",
    "    repo_id=\"weber50432/lora-Llama-3.2-3B-Instruct\",\n",
    "    folder_path=\"../models/lora-Llama-3.2-3B-Instruct\",\n",
    "    path_in_repo=\".\",  # ä¸Šå‚³æ•´åŒ…å…§å®¹åˆ°æ ¹ç›®éŒ„\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your model repository ID\n",
    "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# read API token from json file\n",
    "with open(\"hg-api-key.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    token = data[\"key\"]\n",
    "\n",
    "\n",
    "def call_hf(prompt, max_new_tokens, temperature=0):\n",
    "    \"\"\"\n",
    "    Wrapper function to query Hugging Face Inference API.\n",
    "    \"\"\"\n",
    "    client_hf = InferenceClient(api_key=token)\n",
    "\n",
    "    response = client_hf.chat_completion(\n",
    "        model=repo_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_new_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question of the meaning of life is one of the most profound and debated topics in human history. It has been explored by philosophers, theologians, scientists, and many others across various cultures and disciplines. While there is no one definitive answer, here'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the meaning of life?\"\n",
    "max_new_tokens = 50\n",
    "call_hf(prompt, max_new_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
