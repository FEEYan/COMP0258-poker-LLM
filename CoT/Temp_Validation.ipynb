{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Running experiment at temperature: 0.4\n",
      "‚Üí [0.4] Example 1/100\n",
      "‚Üí [0.4] Example 2/100\n",
      "‚Üí [0.4] Example 3/100\n",
      "‚Üí [0.4] Example 4/100\n",
      "‚Üí [0.4] Example 5/100\n",
      "‚Üí [0.4] Example 6/100\n",
      "‚Üí [0.4] Example 7/100\n",
      "‚Üí [0.4] Example 8/100\n",
      "‚Üí [0.4] Example 9/100\n",
      "‚Üí [0.4] Example 10/100\n",
      "‚Üí [0.4] Example 11/100\n",
      "‚Üí [0.4] Example 12/100\n",
      "‚Üí [0.4] Example 13/100\n",
      "‚Üí [0.4] Example 14/100\n",
      "‚Üí [0.4] Example 15/100\n",
      "‚Üí [0.4] Example 16/100\n",
      "‚Üí [0.4] Example 17/100\n",
      "‚Üí [0.4] Example 18/100\n",
      "‚Üí [0.4] Example 19/100\n",
      "‚Üí [0.4] Example 20/100\n",
      "‚Üí [0.4] Example 21/100\n",
      "‚Üí [0.4] Example 22/100\n",
      "‚Üí [0.4] Example 23/100\n",
      "‚Üí [0.4] Example 24/100\n",
      "‚Üí [0.4] Example 25/100\n",
      "‚Üí [0.4] Example 26/100\n",
      "‚Üí [0.4] Example 27/100\n",
      "‚Üí [0.4] Example 28/100\n",
      "‚Üí [0.4] Example 29/100\n",
      "‚Üí [0.4] Example 30/100\n",
      "‚Üí [0.4] Example 31/100\n",
      "‚Üí [0.4] Example 32/100\n",
      "‚Üí [0.4] Example 33/100\n",
      "‚Üí [0.4] Example 34/100\n",
      "‚Üí [0.4] Example 35/100\n",
      "‚Üí [0.4] Example 36/100\n",
      "‚Üí [0.4] Example 37/100\n",
      "‚Üí [0.4] Example 38/100\n",
      "‚Üí [0.4] Example 39/100\n",
      "‚Üí [0.4] Example 40/100\n",
      "‚Üí [0.4] Example 41/100\n",
      "‚Üí [0.4] Example 42/100\n",
      "‚Üí [0.4] Example 43/100\n",
      "‚Üí [0.4] Example 44/100\n",
      "‚Üí [0.4] Example 45/100\n",
      "‚Üí [0.4] Example 46/100\n",
      "‚Üí [0.4] Example 47/100\n",
      "‚Üí [0.4] Example 48/100\n",
      "‚Üí [0.4] Example 49/100\n",
      "‚Üí [0.4] Example 50/100\n",
      "‚Üí [0.4] Example 51/100\n",
      "‚Üí [0.4] Example 52/100\n",
      "‚Üí [0.4] Example 53/100\n",
      "‚Üí [0.4] Example 54/100\n",
      "‚Üí [0.4] Example 55/100\n",
      "‚Üí [0.4] Example 56/100\n",
      "‚Üí [0.4] Example 57/100\n",
      "‚Üí [0.4] Example 58/100\n",
      "‚Üí [0.4] Example 59/100\n",
      "‚Üí [0.4] Example 60/100\n",
      "‚Üí [0.4] Example 61/100\n",
      "‚Üí [0.4] Example 62/100\n",
      "‚Üí [0.4] Example 63/100\n",
      "‚Üí [0.4] Example 64/100\n",
      "‚Üí [0.4] Example 65/100\n",
      "‚Üí [0.4] Example 66/100\n",
      "‚Üí [0.4] Example 67/100\n",
      "‚Üí [0.4] Example 68/100\n",
      "‚Üí [0.4] Example 69/100\n",
      "‚Üí [0.4] Example 70/100\n",
      "‚Üí [0.4] Example 71/100\n",
      "‚Üí [0.4] Example 72/100\n",
      "‚Üí [0.4] Example 73/100\n",
      "‚Üí [0.4] Example 74/100\n",
      "‚Üí [0.4] Example 75/100\n",
      "‚Üí [0.4] Example 76/100\n",
      "‚Üí [0.4] Example 77/100\n",
      "‚Üí [0.4] Example 78/100\n",
      "‚Üí [0.4] Example 79/100\n",
      "‚Üí [0.4] Example 80/100\n",
      "‚Üí [0.4] Example 81/100\n",
      "‚Üí [0.4] Example 82/100\n",
      "‚Üí [0.4] Example 83/100\n",
      "‚Üí [0.4] Example 84/100\n",
      "‚Üí [0.4] Example 85/100\n",
      "‚Üí [0.4] Example 86/100\n",
      "‚Üí [0.4] Example 87/100\n",
      "‚Üí [0.4] Example 88/100\n",
      "‚Üí [0.4] Example 89/100\n",
      "‚Üí [0.4] Example 90/100\n",
      "‚Üí [0.4] Example 91/100\n",
      "‚Üí [0.4] Example 92/100\n",
      "‚Üí [0.4] Example 93/100\n",
      "‚Üí [0.4] Example 94/100\n",
      "‚Üí [0.4] Example 95/100\n",
      "‚Üí [0.4] Example 96/100\n",
      "‚Üí [0.4] Example 97/100\n",
      "‚Üí [0.4] Example 98/100\n",
      "‚Üí [0.4] Example 99/100\n",
      "‚Üí [0.4] Example 100/100\n",
      "‚úÖ Saved: Poker_Llama-31-8B-Instruct_temp_04.csv\n",
      "\n",
      "üîÅ Running experiment at temperature: 0.8\n",
      "‚Üí [0.8] Example 1/100\n",
      "‚Üí [0.8] Example 2/100\n",
      "‚Üí [0.8] Example 3/100\n",
      "‚Üí [0.8] Example 4/100\n",
      "‚Üí [0.8] Example 5/100\n",
      "‚Üí [0.8] Example 6/100\n",
      "‚Üí [0.8] Example 7/100\n",
      "‚Üí [0.8] Example 8/100\n",
      "‚Üí [0.8] Example 9/100\n",
      "‚Üí [0.8] Example 10/100\n",
      "‚Üí [0.8] Example 11/100\n",
      "‚Üí [0.8] Example 12/100\n",
      "‚Üí [0.8] Example 13/100\n",
      "‚Üí [0.8] Example 14/100\n",
      "‚Üí [0.8] Example 15/100\n",
      "‚Üí [0.8] Example 16/100\n",
      "‚Üí [0.8] Example 17/100\n",
      "‚Üí [0.8] Example 18/100\n",
      "‚Üí [0.8] Example 19/100\n",
      "‚Üí [0.8] Example 20/100\n",
      "‚Üí [0.8] Example 21/100\n",
      "‚Üí [0.8] Example 22/100\n",
      "‚Üí [0.8] Example 23/100\n",
      "‚Üí [0.8] Example 24/100\n",
      "‚Üí [0.8] Example 25/100\n",
      "‚Üí [0.8] Example 26/100\n",
      "‚Üí [0.8] Example 27/100\n",
      "‚Üí [0.8] Example 28/100\n",
      "‚Üí [0.8] Example 29/100\n",
      "‚Üí [0.8] Example 30/100\n",
      "‚Üí [0.8] Example 31/100\n",
      "‚Üí [0.8] Example 32/100\n",
      "‚Üí [0.8] Example 33/100\n",
      "‚Üí [0.8] Example 34/100\n",
      "‚Üí [0.8] Example 35/100\n",
      "‚Üí [0.8] Example 36/100\n",
      "‚Üí [0.8] Example 37/100\n",
      "‚Üí [0.8] Example 38/100\n",
      "‚Üí [0.8] Example 39/100\n",
      "‚Üí [0.8] Example 40/100\n",
      "‚Üí [0.8] Example 41/100\n",
      "‚Üí [0.8] Example 42/100\n",
      "‚Üí [0.8] Example 43/100\n",
      "‚Üí [0.8] Example 44/100\n",
      "‚Üí [0.8] Example 45/100\n",
      "‚Üí [0.8] Example 46/100\n",
      "‚Üí [0.8] Example 47/100\n",
      "‚Üí [0.8] Example 48/100\n",
      "‚Üí [0.8] Example 49/100\n",
      "‚Üí [0.8] Example 50/100\n",
      "‚Üí [0.8] Example 51/100\n",
      "‚Üí [0.8] Example 52/100\n",
      "‚Üí [0.8] Example 53/100\n",
      "‚Üí [0.8] Example 54/100\n",
      "‚Üí [0.8] Example 55/100\n",
      "‚Üí [0.8] Example 56/100\n",
      "‚Üí [0.8] Example 57/100\n",
      "‚Üí [0.8] Example 58/100\n",
      "‚Üí [0.8] Example 59/100\n",
      "‚Üí [0.8] Example 60/100\n",
      "‚Üí [0.8] Example 61/100\n",
      "‚Üí [0.8] Example 62/100\n",
      "‚Üí [0.8] Example 63/100\n",
      "‚Üí [0.8] Example 64/100\n",
      "‚Üí [0.8] Example 65/100\n",
      "‚Üí [0.8] Example 66/100\n",
      "‚Üí [0.8] Example 67/100\n",
      "‚Üí [0.8] Example 68/100\n",
      "‚Üí [0.8] Example 69/100\n",
      "‚Üí [0.8] Example 70/100\n",
      "‚Üí [0.8] Example 71/100\n",
      "‚Üí [0.8] Example 72/100\n",
      "‚Üí [0.8] Example 73/100\n",
      "‚Üí [0.8] Example 74/100\n",
      "‚Üí [0.8] Example 75/100\n",
      "‚Üí [0.8] Example 76/100\n",
      "‚Üí [0.8] Example 77/100\n",
      "‚Üí [0.8] Example 78/100\n",
      "‚Üí [0.8] Example 79/100\n",
      "‚Üí [0.8] Example 80/100\n",
      "‚Üí [0.8] Example 81/100\n",
      "‚Üí [0.8] Example 82/100\n",
      "‚Üí [0.8] Example 83/100\n",
      "‚Üí [0.8] Example 84/100\n",
      "‚Üí [0.8] Example 85/100\n",
      "‚Üí [0.8] Example 86/100\n",
      "‚Üí [0.8] Example 87/100\n",
      "‚Üí [0.8] Example 88/100\n",
      "‚Üí [0.8] Example 89/100\n",
      "‚Üí [0.8] Example 90/100\n",
      "‚Üí [0.8] Example 91/100\n",
      "‚Üí [0.8] Example 92/100\n",
      "‚Üí [0.8] Example 93/100\n",
      "‚Üí [0.8] Example 94/100\n",
      "‚Üí [0.8] Example 95/100\n",
      "‚Üí [0.8] Example 96/100\n",
      "‚Üí [0.8] Example 97/100\n",
      "‚Üí [0.8] Example 98/100\n",
      "‚Üí [0.8] Example 99/100\n",
      "‚Üí [0.8] Example 100/100\n",
      "‚úÖ Saved: Poker_Llama-31-8B-Instruct_temp_08.csv\n"
     ]
    }
   ],
   "source": [
    "# Temperature experiments\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Set your Hugging Face API token here\n",
    "hf_token = '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'\n",
    "\n",
    "def main(model):\n",
    "    model_name = model.split(\"/\")[-1].replace(\".\", \"\")\n",
    "\n",
    "    # Create an Inference API instance for the model\n",
    "    def call_hf(model, messages, temperature=0.0):\n",
    "        '''\n",
    "        Function to query Hugging Face Inference API.\n",
    "        '''\n",
    "        client_hf = InferenceClient(api_key=hf_token)\n",
    "        response = client_hf.chat_completion(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=1024,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    # Set the system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "            You are an expert at Texas Holdem Poker. Your job is to do step-by-step reasoning about a game scenario and then provide a final answer as 'Final Answer:'.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    def get_optimal_action(text, temperature=0.0):\n",
    "        '''\n",
    "        Sends a prompt to the Hugging Face Inference API and returns the response.\n",
    "        '''\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text\n",
    "        }\n",
    "        messages = [system_message, user_message]\n",
    "        response = call_hf(model, messages, temperature)\n",
    "        return response\n",
    "\n",
    "    def parse_output(output_text):\n",
    "        '''\n",
    "        Function to parse the output from the model.\n",
    "        '''\n",
    "        if \"Final Answer:\" in output_text:\n",
    "            parts = output_text.split(\"Final Answer:\")\n",
    "            reasoning = parts[0].strip()\n",
    "            final_answer = parts[1].strip().splitlines()[0]\n",
    "        elif \"optimal action:\" in output_text.lower():\n",
    "            parts = re.split(r\"optimal action:\", output_text, flags=re.IGNORECASE)\n",
    "            reasoning = parts[0].strip()\n",
    "            final_answer = parts[1].strip().splitlines()[0]\n",
    "        else:\n",
    "            reasoning = output_text\n",
    "            final_answer = \"\"\n",
    "        return reasoning, final_answer\n",
    "\n",
    "    # Load datasets\n",
    "    with open(\"Postflop 100 Sample.json\", \"r\") as f:\n",
    "        postflop_test_set = json.load(f)\n",
    "    with open(\"Preflop 100 Sample.json\", \"r\") as f:\n",
    "        preflop_test_set = json.load(f)\n",
    "    all_examples = postflop_test_set\n",
    "\n",
    "    # CoT prompt setup\n",
    "    extra_instruction = '''\n",
    "    You are an expert in 6-handed No Limit Texas Holdem. Your job is to analyze a game scenario and decide on the optimal action.\n",
    "\n",
    "    Think through your answer step-by-step and then output exactly one sentence starting with \"Final Answer:\"\n",
    "    '''\n",
    "\n",
    "    # Run across multiple temperatures\n",
    "    for temperature in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "        print(f\"\\n Running experiment at temperature: {temperature}\")\n",
    "        results = []\n",
    "\n",
    "        for idx, example in enumerate(all_examples):\n",
    "            prompt_text = example.get(\"instruction\", \"\")\n",
    "            prompt_text = prompt_text.replace(\"Do not explain your answer.\", \"\").strip()\n",
    "            prompt_text = extra_instruction + prompt_text\n",
    "\n",
    "            print(f\"‚Üí [{temperature}] Example {idx+1}/{len(all_examples)}\")\n",
    "\n",
    "            try:\n",
    "                output = get_optimal_action(prompt_text, temperature)\n",
    "                reasoning, final_answer = parse_output(output)\n",
    "            except Exception as e:\n",
    "                print(f\"Error on example {idx+1}: {e}\")\n",
    "                output = reasoning = final_answer = \"Error\"\n",
    "\n",
    "            results.append({\n",
    "                \"instruction\": example.get(\"instruction\", \"\"),\n",
    "                \"prompt\": prompt_text,\n",
    "                \"ground_truth\": example.get(\"output\", \"\"),\n",
    "                \"reasoning\": output,\n",
    "                \"final_answer\": final_answer\n",
    "            })\n",
    "\n",
    "            time.sleep(1)  # Respect rate limits\n",
    "\n",
    "        # Save output to CSV\n",
    "        out_path = f\"Poker_{model_name}_temp_{str(temperature).replace('.', '')}.csv\"\n",
    "        pd.DataFrame(results).to_csv(out_path, index=False)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "# Run it for Llama (or replace with other text-gen model name)\n",
    "main(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
