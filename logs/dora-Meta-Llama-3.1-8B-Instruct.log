Start time: Wed 12 Feb 2025 23:12:47 GMT
Loading pretrained model
Fetching 11 files:   0%|                                                                                   | 0/11 [00:00<?, ?it/s]Fetching 11 files: 100%|███████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 56402.62it/s]
Loading datasets
Training
Trainable parameters: 0.044% (3.572M/8030.261M)
Starting training..., iters: 5000
Iter 1: Val loss 2.861, Val took 35.036s
Iter 100: Val loss 1.837, Val took 37.158s
Iter 100: Train loss 2.261, Learning Rate 1.000e-06, It/sec 35.399, Tokens/sec 22383.570, Trained Tokens 63232, Peak mem 22.784 GB
Iter 100: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000100_adapters.safetensors.
Iter 200: Val loss 1.129, Val took 31.609s
Iter 200: Train loss 1.501, Learning Rate 1.000e-06, It/sec 38.205, Tokens/sec 24166.684, Trained Tokens 126488, Peak mem 22.858 GB
Iter 200: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000200_adapters.safetensors.
Iter 300: Val loss 0.582, Val took 42.284s
Iter 300: Train loss 0.828, Learning Rate 1.000e-06, It/sec 14.877, Tokens/sec 9483.313, Trained Tokens 190234, Peak mem 23.024 GB
Iter 300: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000300_adapters.safetensors.
Iter 400: Val loss 0.349, Val took 33.969s
Iter 400: Train loss 0.450, Learning Rate 1.000e-06, It/sec 30.297, Tokens/sec 19444.107, Trained Tokens 254412, Peak mem 23.024 GB
Iter 400: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000400_adapters.safetensors.
Iter 500: Val loss 0.285, Val took 35.875s
Iter 500: Train loss 0.314, Learning Rate 1.000e-06, It/sec 38.394, Tokens/sec 24358.568, Trained Tokens 317856, Peak mem 23.024 GB
Iter 500: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000500_adapters.safetensors.
Iter 600: Val loss 0.254, Val took 33.183s
Iter 600: Train loss 0.259, Learning Rate 1.000e-06, It/sec 38.179, Tokens/sec 24188.940, Trained Tokens 381212, Peak mem 23.024 GB
Iter 600: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000600_adapters.safetensors.
Iter 700: Val loss 0.231, Val took 32.761s
Iter 700: Train loss 0.246, Learning Rate 1.000e-06, It/sec 37.618, Tokens/sec 23762.041, Trained Tokens 444378, Peak mem 23.024 GB
Iter 700: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000700_adapters.safetensors.
Iter 800: Val loss 0.228, Val took 84.311s
Iter 800: Train loss 0.226, Learning Rate 1.000e-06, It/sec 40.025, Tokens/sec 25326.779, Trained Tokens 507656, Peak mem 23.024 GB
Iter 800: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000800_adapters.safetensors.
Iter 900: Val loss 0.220, Val took 34.399s
Iter 900: Train loss 0.221, Learning Rate 1.000e-06, It/sec 37.795, Tokens/sec 23743.766, Trained Tokens 570478, Peak mem 23.024 GB
Iter 900: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0000900_adapters.safetensors.
Iter 1000: Val loss 0.207, Val took 35.637s
Iter 1000: Train loss 0.206, Learning Rate 1.000e-06, It/sec 38.203, Tokens/sec 24575.520, Trained Tokens 634807, Peak mem 23.024 GB
Iter 1000: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001000_adapters.safetensors.
Iter 1100: Val loss 0.194, Val took 47.809s
Iter 1100: Train loss 0.206, Learning Rate 1.000e-06, It/sec 26.244, Tokens/sec 16541.044, Trained Tokens 697835, Peak mem 23.024 GB
Iter 1100: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001100_adapters.safetensors.
Iter 1200: Val loss 0.198, Val took 81.166s
Iter 1200: Train loss 0.197, Learning Rate 1.000e-06, It/sec 37.554, Tokens/sec 23857.155, Trained Tokens 761363, Peak mem 23.024 GB
Iter 1200: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001200_adapters.safetensors.
Iter 1300: Val loss 0.191, Val took 35.114s
Iter 1300: Train loss 0.190, Learning Rate 1.000e-06, It/sec 37.385, Tokens/sec 23931.839, Trained Tokens 825377, Peak mem 23.024 GB
Iter 1300: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001300_adapters.safetensors.
Iter 1400: Val loss 0.184, Val took 44.926s
Iter 1400: Train loss 0.187, Learning Rate 1.000e-06, It/sec 15.494, Tokens/sec 9989.007, Trained Tokens 889846, Peak mem 23.024 GB
Iter 1400: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001400_adapters.safetensors.
Iter 1500: Val loss 0.183, Val took 63.698s
Iter 1500: Train loss 0.180, Learning Rate 1.000e-06, It/sec 21.045, Tokens/sec 13380.595, Trained Tokens 953428, Peak mem 23.024 GB
Iter 1500: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001500_adapters.safetensors.
Iter 1600: Val loss 0.177, Val took 34.431s
Iter 1600: Train loss 0.180, Learning Rate 1.000e-06, It/sec 20.256, Tokens/sec 12910.721, Trained Tokens 1017166, Peak mem 23.024 GB
Iter 1600: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001600_adapters.safetensors.
Iter 1700: Val loss 0.173, Val took 31.537s
Iter 1700: Train loss 0.175, Learning Rate 1.000e-06, It/sec 49.748, Tokens/sec 31782.825, Trained Tokens 1081054, Peak mem 23.024 GB
Iter 1700: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001700_adapters.safetensors.
Iter 1800: Val loss 0.171, Val took 35.556s
Iter 1800: Train loss 0.173, Learning Rate 1.000e-06, It/sec 34.490, Tokens/sec 21831.519, Trained Tokens 1144352, Peak mem 23.024 GB
Iter 1800: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001800_adapters.safetensors.
Iter 1900: Val loss 0.170, Val took 36.378s
Iter 1900: Train loss 0.172, Learning Rate 1.000e-06, It/sec 30.622, Tokens/sec 19703.101, Trained Tokens 1208696, Peak mem 23.024 GB
Iter 1900: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0001900_adapters.safetensors.
Iter 2000: Val loss 0.161, Val took 35.687s
Iter 2000: Train loss 0.168, Learning Rate 1.000e-06, It/sec 36.196, Tokens/sec 23051.576, Trained Tokens 1272382, Peak mem 23.024 GB
Iter 2000: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002000_adapters.safetensors.
Iter 2100: Val loss 0.166, Val took 32.601s
Iter 2100: Train loss 0.163, Learning Rate 1.000e-06, It/sec 36.600, Tokens/sec 23364.819, Trained Tokens 1336220, Peak mem 23.024 GB
Iter 2100: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002100_adapters.safetensors.
Iter 2200: Val loss 0.159, Val took 33.380s
Iter 2200: Train loss 0.165, Learning Rate 1.000e-06, It/sec 37.515, Tokens/sec 23725.869, Trained Tokens 1399464, Peak mem 23.024 GB
Iter 2200: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002200_adapters.safetensors.
Iter 2300: Val loss 0.158, Val took 98.125s
Iter 2300: Train loss 0.160, Learning Rate 1.000e-06, It/sec 37.711, Tokens/sec 24114.597, Trained Tokens 1463410, Peak mem 23.024 GB
Iter 2300: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002300_adapters.safetensors.
Iter 2400: Val loss 0.161, Val took 33.509s
Iter 2400: Train loss 0.161, Learning Rate 1.000e-06, It/sec 37.873, Tokens/sec 24130.843, Trained Tokens 1527126, Peak mem 23.024 GB
Iter 2400: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002400_adapters.safetensors.
Iter 2500: Val loss 0.163, Val took 37.088s
Iter 2500: Train loss 0.159, Learning Rate 1.000e-06, It/sec 39.886, Tokens/sec 25547.979, Trained Tokens 1591178, Peak mem 23.024 GB
Iter 2500: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002500_adapters.safetensors.
Iter 2600: Val loss 0.157, Val took 33.188s
Iter 2600: Train loss 0.158, Learning Rate 1.000e-06, It/sec 36.868, Tokens/sec 23497.286, Trained Tokens 1654912, Peak mem 23.024 GB
Iter 2600: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002600_adapters.safetensors.
Iter 2700: Val loss 0.156, Val took 73.431s
Iter 2700: Train loss 0.154, Learning Rate 1.000e-06, It/sec 35.700, Tokens/sec 22549.542, Trained Tokens 1718076, Peak mem 23.024 GB
Iter 2700: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002700_adapters.safetensors.
Iter 2800: Val loss 0.157, Val took 36.649s
Iter 2800: Train loss 0.153, Learning Rate 1.000e-06, It/sec 37.338, Tokens/sec 23548.947, Trained Tokens 1781146, Peak mem 23.024 GB
Iter 2800: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002800_adapters.safetensors.
Iter 2900: Val loss 0.148, Val took 48.694s
Iter 2900: Train loss 0.151, Learning Rate 1.000e-06, It/sec 35.662, Tokens/sec 22564.951, Trained Tokens 1844420, Peak mem 23.024 GB
Iter 2900: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0002900_adapters.safetensors.
Iter 3000: Val loss 0.151, Val took 34.861s
Iter 3000: Train loss 0.153, Learning Rate 1.000e-06, It/sec 36.490, Tokens/sec 23372.378, Trained Tokens 1908472, Peak mem 23.024 GB
Iter 3000: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003000_adapters.safetensors.
Iter 3100: Val loss 0.151, Val took 31.740s
Iter 3100: Train loss 0.151, Learning Rate 1.000e-06, It/sec 36.875, Tokens/sec 23491.359, Trained Tokens 1972178, Peak mem 23.024 GB
Iter 3100: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003100_adapters.safetensors.
Iter 3200: Val loss 0.150, Val took 36.618s
Iter 3200: Train loss 0.152, Learning Rate 1.000e-06, It/sec 34.505, Tokens/sec 22018.498, Trained Tokens 2035990, Peak mem 23.024 GB
Iter 3200: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003200_adapters.safetensors.
Iter 3300: Val loss 0.147, Val took 33.668s
Iter 3300: Train loss 0.152, Learning Rate 1.000e-06, It/sec 37.091, Tokens/sec 23383.570, Trained Tokens 2099034, Peak mem 23.024 GB
Iter 3300: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003300_adapters.safetensors.
Iter 3400: Val loss 0.145, Val took 35.466s
Iter 3400: Train loss 0.147, Learning Rate 1.000e-06, It/sec 36.501, Tokens/sec 23513.944, Trained Tokens 2163454, Peak mem 23.024 GB
Iter 3400: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003400_adapters.safetensors.
Iter 3500: Val loss 0.149, Val took 34.822s
Iter 3500: Train loss 0.149, Learning Rate 1.000e-06, It/sec 39.801, Tokens/sec 25494.374, Trained Tokens 2227508, Peak mem 23.024 GB
Iter 3500: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003500_adapters.safetensors.
Iter 3600: Val loss 0.150, Val took 36.803s
Iter 3600: Train loss 0.145, Learning Rate 1.000e-06, It/sec 39.041, Tokens/sec 24939.707, Trained Tokens 2291388, Peak mem 23.024 GB
Iter 3600: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003600_adapters.safetensors.
Iter 3700: Val loss 0.145, Val took 35.081s
Iter 3700: Train loss 0.145, Learning Rate 1.000e-06, It/sec 36.656, Tokens/sec 23664.817, Trained Tokens 2355948, Peak mem 23.024 GB
Iter 3700: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003700_adapters.safetensors.
Iter 3800: Val loss 0.144, Val took 100.833s
Iter 3800: Train loss 0.142, Learning Rate 1.000e-06, It/sec 27.065, Tokens/sec 17147.039, Trained Tokens 2419304, Peak mem 23.024 GB
Iter 3800: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003800_adapters.safetensors.
Iter 3900: Val loss 0.139, Val took 31.738s
Iter 3900: Train loss 0.140, Learning Rate 1.000e-06, It/sec 37.906, Tokens/sec 23679.639, Trained Tokens 2481774, Peak mem 23.024 GB
Iter 3900: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0003900_adapters.safetensors.
Iter 4000: Val loss 0.142, Val took 33.433s
Iter 4000: Train loss 0.141, Learning Rate 1.000e-06, It/sec 43.948, Tokens/sec 27679.462, Trained Tokens 2544756, Peak mem 23.024 GB
Iter 4000: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004000_adapters.safetensors.
Iter 4100: Val loss 0.137, Val took 32.593s
Iter 4100: Train loss 0.140, Learning Rate 1.000e-06, It/sec 40.239, Tokens/sec 25284.646, Trained Tokens 2607592, Peak mem 23.024 GB
Iter 4100: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004100_adapters.safetensors.
Iter 4200: Val loss 0.144, Val took 91.391s
Iter 4200: Train loss 0.139, Learning Rate 1.000e-06, It/sec 35.791, Tokens/sec 22664.402, Trained Tokens 2670916, Peak mem 23.024 GB
Iter 4200: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004200_adapters.safetensors.
Iter 4300: Val loss 0.137, Val took 32.735s
Iter 4300: Train loss 0.139, Learning Rate 1.000e-06, It/sec 37.212, Tokens/sec 23651.078, Trained Tokens 2734474, Peak mem 23.024 GB
Iter 4300: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004300_adapters.safetensors.
Iter 4400: Val loss 0.144, Val took 33.821s
Iter 4400: Train loss 0.136, Learning Rate 1.000e-06, It/sec 43.524, Tokens/sec 27438.319, Trained Tokens 2797516, Peak mem 23.024 GB
Iter 4400: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004400_adapters.safetensors.
Iter 4500: Val loss 0.140, Val took 35.378s
Iter 4500: Train loss 0.139, Learning Rate 1.000e-06, It/sec 40.509, Tokens/sec 26086.754, Trained Tokens 2861914, Peak mem 23.024 GB
Iter 4500: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004500_adapters.safetensors.
Iter 4600: Val loss 0.136, Val took 32.018s
Iter 4600: Train loss 0.138, Learning Rate 1.000e-06, It/sec 40.709, Tokens/sec 26133.445, Trained Tokens 2926110, Peak mem 23.024 GB
Iter 4600: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004600_adapters.safetensors.
Iter 4700: Val loss 0.140, Val took 34.729s
Iter 4700: Train loss 0.137, Learning Rate 1.000e-06, It/sec 34.302, Tokens/sec 21736.578, Trained Tokens 2989478, Peak mem 23.024 GB
Iter 4700: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004700_adapters.safetensors.
Iter 4800: Val loss 0.135, Val took 33.733s
Iter 4800: Train loss 0.137, Learning Rate 1.000e-06, It/sec 39.701, Tokens/sec 25217.828, Trained Tokens 3052998, Peak mem 23.024 GB
Iter 4800: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004800_adapters.safetensors.
Iter 4900: Val loss 0.133, Val took 33.018s
Iter 4900: Train loss 0.133, Learning Rate 1.000e-06, It/sec 35.628, Tokens/sec 22538.286, Trained Tokens 3116258, Peak mem 23.024 GB
Iter 4900: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0004900_adapters.safetensors.
Iter 5000: Val loss 0.138, Val took 32.464s
Iter 5000: Train loss 0.134, Learning Rate 1.000e-06, It/sec 37.739, Tokens/sec 24209.632, Trained Tokens 3180408, Peak mem 23.024 GB
Iter 5000: Saved adapter weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors and ../adapters/dora-Meta-Llama-3.1-8B-Instruct/0005000_adapters.safetensors.
Saved final weights to ../adapters/dora-Meta-Llama-3.1-8B-Instruct/adapters.safetensors.
Testing
Test loss 0.135, Test ppl 1.145.
End time: Thu 13 Feb 2025 04:07:57 GMT
